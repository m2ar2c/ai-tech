<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>幻灯片 10 - 自注意力与 Transformer</title>
    <style>
        :root {
            --primary: #165DFF;
            --secondary: #36CFFB;
            --accent: #722ED1;
            --bg: #050A20;
            --surface: rgba(8, 20, 54, 0.8);
            --text-main: #F4F8FF;
            --text-secondary: #A4B7DB;
            --border: rgba(255, 255, 255, 0.08);
        }

        * {
            box-sizing: border-box;
        }

        html,
        body {
            height: 100%;
        }

        body {
            margin: 0;
            font-family: 'Microsoft YaHei', 'Noto Sans SC', 'PingFang SC', sans-serif;
            background: var(--bg);
            color: var(--text-main);
            display: flex;
            align-items: center;
            justify-content: center;
            height: 100vh;
            overflow: hidden;
            --scale: min(calc(100vw / 1280), calc(100vh / 720));
        }

        .scale-wrapper {
            width: 1280px;
            height: 720px;
            transform: scale(var(--scale));
            transform-origin: center center;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .slide {
            width: 1280px;
            height: 720px;
            border-radius: 28px;
            padding: 70px 86px;
            background: linear-gradient(145deg, rgba(9, 18, 46, 0.96) 0%, rgba(12, 34, 82, 0.92) 50%, rgba(6, 16, 36, 0.95) 100%);
            position: relative;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            isolation: isolate;
        }

        .slide::before {
            content: "";
            position: absolute;
            inset: 22px;
            border-radius: 22px;
            border: 1px solid rgba(255, 255, 255, 0.06);
            pointer-events: none;
        }

        header {
            margin-bottom: 36px;
        }

        header h1 {
            margin: 0 0 12px;
            font-size: 48px;
        }

        header p {
            margin: 0;
            font-size: 22px;
            color: var(--text-secondary);
            max-width: 780px;
            line-height: 1.6;
        }

        main {
            flex: 1;
            display: grid;
            grid-template-columns: 0.9fr 1.1fr;
            gap: 32px;
        }

        .attention-card,
        .transformer-card {
            background: var(--surface);
            border-radius: 22px;
            border: 1px solid var(--border);
            padding: 30px 32px;
            display: flex;
            flex-direction: column;
            gap: 20px;
            backdrop-filter: blur(14px);
        }

        .attention-card h2,
        .transformer-card h2 {
            margin: 0;
            font-size: 30px;
            display: flex;
            align-items: center;
            gap: 12px;
        }

        .attention-card h2 svg,
        .transformer-card h2 svg {
            width: 26px;
            height: 26px;
            fill: var(--secondary);
        }

        .formula-box {
            background: rgba(6, 16, 36, 0.72);
            border-radius: 18px;
            border: 1px solid rgba(54, 207, 251, 0.22);
            padding: 18px 20px;
            font-size: 22px;
            line-height: 1.8;
            font-family: 'Fira Code', 'JetBrains Mono', monospace;
        }

        .attention-card ul,
        .transformer-card ul {
            list-style: none;
            margin: 0;
            padding: 0;
            display: flex;
            flex-direction: column;
            gap: 14px;
        }

        .attention-card li,
        .transformer-card li {
            font-size: 19px;
            color: var(--text-secondary);
            line-height: 1.6;
            position: relative;
            padding-left: 24px;
        }

        .attention-card li::before,
        .transformer-card li::before {
            content: "";
            position: absolute;
            left: 0;
            top: 9px;
            width: 10px;
            height: 10px;
            border-radius: 50%;
            background: linear-gradient(135deg, var(--secondary), var(--primary));
        }

        .multihead-grid {
            display: grid;
            grid-template-columns: repeat(3, minmax(0, 1fr));
            gap: 12px;
        }

        .head-box {
            background: rgba(114, 46, 209, 0.18);
            border-radius: 16px;
            border: 1px solid rgba(114, 46, 209, 0.28);
            padding: 14px 16px;
            font-size: 18px;
            color: var(--text-secondary);
            line-height: 1.5;
        }

        .architecture-flow {
            display: grid;
            grid-template-columns: repeat(4, minmax(0, 1fr));
            gap: 12px;
        }

        .flow-step {
            background: rgba(6, 16, 36, 0.7);
            border-radius: 16px;
            border: 1px solid rgba(22, 93, 255, 0.2);
            padding: 16px 18px;
            font-size: 18px;
            color: var(--text-secondary);
            line-height: 1.5;
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .flow-step strong {
            font-size: 20px;
            color: var(--text-main);
        }

        footer {
            margin-top: 26px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            font-size: 18px;
            color: var(--text-secondary);
        }

        .page-number {
            font-size: 20px;
            color: var(--secondary);
        }
    </style>
</head>
<body>
    <div class="scale-wrapper">
        <article class="slide" role="article" aria-label="自注意力与 Transformer 机制">
            <header>
                <h1>自注意力机制与 Transformer 架构</h1>
                <p>Transformer 通过多头自注意力与前馈网络组合，摒弃循环结构，实现长距离依赖建模与高并行训练。</p>
            </header>
            <main>
                <section class="attention-card" aria-label="自注意力原理">
                    <h2>
                        <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" aria-hidden="true">
                            <path d="M12 2a10 10 0 1 0 10 10A10.011 10.011 0 0 0 12 2Zm0 18a8 8 0 1 1 8-8 8.009 8.009 0 0 1-8 8Zm3-10h2v2h-2v2h-2v-2h-2v-2h2V8h2Z" />
                        </svg>
                        自注意力核心公式
                    </h2>
                    <div class="formula-box">Attention(Q, K, V) = softmax\left(\frac{QK^{T}}{\sqrt{d_k}}\right) V</div>
                    <ul>
                        <li>Query、Key、Value 对应当前词向量的查询、匹配与信息提取三个角色。</li>
                        <li>通过缩放点积计算相似度，再经 softmax 获得注意力权重，以加权求和输出。</li>
                        <li>支持并行处理序列中任意位置的依赖关系，突破 RNN 长程依赖瓶颈。</li>
                    </ul>
                    <div class="multihead-grid" aria-label="多头自注意力优势">
                        <div class="head-box">多头机制让模型在不同子空间捕捉语法、语义与位置信息。</div>
                        <div class="head-box">每个头执行独立注意力，再拼接融合，增强表达能力。</div>
                        <div class="head-box">Residual + LayerNorm 保证训练稳定性，实现更深网络堆叠。</div>
                    </div>
                </section>
                <section class="transformer-card" aria-label="Transformer 架构">
                    <h2>
                        <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" aria-hidden="true">
                            <path d="M12 2 2 7v10l10 5 10-5V7Zm0 2.18 6.764 3.382L12 10.944 5.236 7.562ZM4 9.618l7 3.5v7.264l-7-3.5Zm16 0v7.264l-7 3.5v-7.264Z" />
                        </svg>
                        编码器-解码器流程
                    </h2>
                    <ul>
                        <li>编码器堆叠多层（自注意力 + 前馈网络），输入嵌入加位置编码以保留序列顺序。</li>
                        <li>解码器包含 Masked Self-Attention，避免看到未来词，同时与编码器输出进行交互注意力。</li>
                        <li>前馈网络采用两层全连接 + 激活，提升非线性表达能力。</li>
                    </ul>
                    <div class="architecture-flow" aria-label="Transformer 流程">
                        <div class="flow-step">
                            <strong>输入嵌入</strong>
                            <span>词向量 + 位置编码</span>
                        </div>
                        <div class="flow-step">
                            <strong>多头自注意力</strong>
                            <span>捕捉全局依赖</span>
                        </div>
                        <div class="flow-step">
                            <strong>前馈网络</strong>
                            <span>逐位置变换</span>
                        </div>
                        <div class="flow-step">
                            <strong>残差 + LayerNorm</strong>
                            <span>稳定训练</span>
                        </div>
                    </div>
                    <h2>
                        <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" aria-hidden="true">
                            <path d="M5 4h14v2H5Zm0 7h14v2H5Zm0 7h14v2H5Z" />
                        </svg>
                        应用价值
                    </h2>
                    <ul>
                        <li>自注意力提供 O(n²) 复杂度，配合 GPU 并行显著提升训练效率。</li>
                        <li>适配多模态扩展（视觉、语音），成为大模型通用框架。</li>
                        <li>结合 LoRA、QLoRA 等微调策略，可快速适应行业场景。</li>
                    </ul>
                </section>
            </main>
            <footer>
                <div>提示：在算力充足条件下，自注意力模型可通过多头与残差设计实现更大规模、更稳定的训练。</div>
                <div class="page-number" aria-live="polite"></div>
            </footer>
        </article>
    </div>
    <script src="navigation.js"></script>
</body>
</html>
