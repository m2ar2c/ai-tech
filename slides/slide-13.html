<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI 技术核心解析报告 - 幻灯片 13</title>
    <style>
        :root { color-scheme: light; }
        * { box-sizing: border-box; }
        body {
            margin: 0;
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            background: radial-gradient(circle at 20% 20%, #16213E 0%, #0F172A 45%, #020617 100%);
            font-family: 'HarmonyOS Sans SC', 'PingFang SC', 'Microsoft YaHei', 'Source Han Sans SC', sans-serif;
            color: #1D2939;
        }
        .slide {
            width: 1280px;
            height: 720px;
            background: #FFFFFF;
            border-radius: 32px;
            position: relative;
            padding: 96px 112px 120px 112px;
            box-shadow: 0 32px 120px rgba(15, 23, 42, 0.45);
            overflow: hidden;
        }
        .slide.cover {
            background: linear-gradient(140deg, #165DFF 0%, #36CFFB 55%, #5AD7F7 100%);
            color: #FFFFFF;
        }
        .slide.cover .badge {
            background: rgba(255, 255, 255, 0.18);
            color: #FFFFFF;
        }
        .slide.cover .slide-title {
            color: #FFFFFF;
            font-size: 64px;
        }
        .slide.cover .slide-subtitle {
            color: rgba(255, 255, 255, 0.85);
            font-size: 28px;
        }
        .slide.cover .content ul li {
            color: rgba(255, 255, 255, 0.9);
        }
        .badge {
            position: absolute;
            top: 36px;
            left: 48px;
            padding: 6px 18px;
            border-radius: 999px;
            background: rgba(22, 93, 255, 0.12);
            color: #165DFF;
            font-size: 18px;
            font-weight: 600;
            letter-spacing: 0.02em;
        }
        .slide-title {
            margin: 0;
            font-size: 46px;
            line-height: 1.2;
            color: #101828;
            font-weight: 700;
            letter-spacing: 0.01em;
        }
        .slide-subtitle {
            margin: 18px 0 48px 0;
            font-size: 24px;
            line-height: 1.6;
            color: #475467;
            max-width: 780px;
        }
        .content {
            font-size: 24px;
            line-height: 1.6;
            color: #1D2939;
        }
        .content ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }
        .content li {
            margin-bottom: 20px;
            position: relative;
            padding-left: 28px;
        }
        .content li::before {
            content: '';
            position: absolute;
            left: 0;
            top: 12px;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #165DFF;
            box-shadow: 0 0 0 5px rgba(22, 93, 255, 0.2);
        }
        .slide.cover .content li::before {
            background: rgba(255, 255, 255, 0.9);
            box-shadow: 0 0 0 5px rgba(255, 255, 255, 0.2);
        }
        .nav-hint {
            position: absolute;
            bottom: 40px;
            left: 56px;
            font-size: 18px;
            color: #98A2B3;
            letter-spacing: 0.02em;
        }
        .slide.cover .nav-hint {
            color: rgba(255, 255, 255, 0.7);
        }
        .page-number {
            position: absolute;
            bottom: 40px;
            right: 56px;
            font-size: 20px;
            color: #98A2B3;
            font-weight: 500;
            letter-spacing: 0.04em;
        }
        .slide.cover .page-number {
            color: rgba(255, 255, 255, 0.75);
        }
    </style>
</head>
<body>
    <div class="slide" data-slide="13">
            <div class="badge">四、大模型自注意力学习机制</div>
            <h1 class="slide-title">4.1 自注意力机制的工作原理</h1>
            <div class="content">
                <ul>
                <li>自注意力机制（Self-Attention）是 Transformer 架构的核心创新，它彻底改变了模型处理序列数据的方式</li>
                <li>与传统的循环神经网络（RNN）和长短期记忆网络（LSTM）不同，自注意力机制允许模型在处理序列中的每个元素时，直接关注序列中的所有其他元素，从而捕获长距离依赖关系</li>
                <li>自注意力机制的核心思想可以用 查询 - 键 - 值（Query-Key-Value） 三元组来理解：查询（Query）：当前元素为了计算自己的新表示</li>
                <li>向其他所有元素发出的 "查询信号"、键（Key）：序列中其他元素为了响应查询而提供的 "标识或标签"、值（Value）：序列中其他元素所携带的 "实际内容或信息"</li>
                <li>自注意力的计算过程可以概括为：</li>
                </ul>
            </div>
            <div class="nav-hint">使用 ← → 键切换</div>
            <div class="page-number"></div>
    </div>
    <script src="navigation.js"></script>
</body>
</html>